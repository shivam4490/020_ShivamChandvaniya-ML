{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab_07_1_LogisticRegression-Tweets.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNjHNzflN53uXYak08gVWt8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"eMAElT0ZOUm9"},"source":["**Aim:**\n","Extract features for logistic regression given some text\n","\n","*   \n","Implement logistic regression from scratch\n","\n","\n","*   Apply logistic regression on a natural language processing task\n","*   Test logistic regression\n","\n","\n","\n","\n","\n","\n","We will be using a data set of tweets.\n","\n","**Import functions and data**"]},{"cell_type":"code","metadata":{"id":"8KeQvHJVOx9K","executionInfo":{"status":"ok","timestamp":1633618282991,"user_tz":-330,"elapsed":335,"user":{"displayName":"CE020_Shivam_ Chandvaniya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03711213958246720098"}}},"source":["import nltk\n","from nltk.corpus import twitter_samples \n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LAc9u_O7PIK9","executionInfo":{"status":"ok","timestamp":1633618296886,"user_tz":-330,"elapsed":1328,"user":{"displayName":"CE020_Shivam_ Chandvaniya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03711213958246720098"}},"outputId":"d10a8547-973f-469b-ea7c-b6f07baaa484"},"source":["nltk.download('twitter_samples')\n","nltk.download('stopwords')"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/twitter_samples.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"Y8mvMrKhPSAs","executionInfo":{"status":"ok","timestamp":1633618312218,"user_tz":-330,"elapsed":349,"user":{"displayName":"CE020_Shivam_ Chandvaniya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03711213958246720098"}}},"source":["import re\n","import string\n","import numpy as np\n","\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import TweetTokenizer"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"gb_eBIVTPUi0","executionInfo":{"status":"ok","timestamp":1633618329778,"user_tz":-330,"elapsed":373,"user":{"displayName":"CE020_Shivam_ Chandvaniya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03711213958246720098"}}},"source":["#process_tweet(): cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\n","def process_tweet(tweet):\n","    \"\"\"Process tweet function.\n","    Input:\n","        tweet: a string containing a tweet\n","    Output:\n","        tweets_clean: a list of words containing the processed tweet\n","\n","    \"\"\"\n","    stemmer = PorterStemmer()\n","    stopwords_english = stopwords.words('english')\n","\n","    # remove stock market tickers like $GE\n","    tweet = re.sub(r'\\$\\w*', '', tweet)\n","    # remove old style retweet text \"RT\"\n","    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n","    # remove hyperlinks\n","    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n","    # remove hashtags\n","    # only removing the hash # sign from the word\n","    tweet = re.sub(r'#', '', tweet)\n","    # tokenize tweets\n","\n","\n","    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n","                               reduce_len=True)\n","    tweet_tokens = tokenizer.tokenize(tweet)\n","\n","    tweets_clean = []\n","    for word in tweet_tokens:\n","      if(word not in stopwords_english and word not in string.punctuation):\n","        stem_word = stemmer.stem(word)\n","        tweets_clean.append(stem_word)\n","            #############################################################\n","            # 1 remove stopwords\n","            # 2 remove punctuation\n","            # 3 stemming word\n","            # 4 Add it to tweets_clean\n","    return tweets_clean"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"7UnbO-a-PY9c","executionInfo":{"status":"ok","timestamp":1633618344182,"user_tz":-330,"elapsed":359,"user":{"displayName":"CE020_Shivam_ Chandvaniya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03711213958246720098"}}},"source":["#build_freqs counts how often a word in the 'corpus' (the entire set of tweets) was associated with\n","  # a positive label '1'         or \n","  # a negative label '0', \n","\n","#then builds the freqs dictionary, where each key is a (word,label) tuple, \n","\n","#and the value is the count of its frequency within the corpus of tweets.\n","\n","def build_freqs(tweets, ys):\n","    \"\"\"Build frequencies.\n","    Input:\n","        tweets: a list of tweets\n","        ys: an m x 1 array with the sentiment label of each tweet\n","            (either 0 or 1)\n","    Output:\n","        freqs: a dictionary mapping each (word, sentiment) pair to its\n","        frequency\n","    \"\"\"\n","    # Convert np array to list since zip needs an iterable.\n","    # The squeeze is necessary or the list ends up with one element.\n","    # Also note that this is just a NOP if ys is already a list.\n","    yslist = np.squeeze(ys).tolist()\n","\n","    # Start with an empty dictionary and populate it by looping over all tweets\n","    # and over all processed words in each tweet.\n","    freqs = {}\n","\n","    for y, tweet in zip(yslist, tweets):\n","        for word in process_tweet(tweet):\n","            pair = (word, y)\n","            \n","            #############################################################\n","            #Update the count of pair if present, set it to 1 otherwise\n","            if pair in freqs:\n","              freqs[pair] += 1\n","            else:\n","              freqs[pair] = 1\n","\n","    return freqs"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kEqEM20APeB5"},"source":["\n","**Prepare the data**\n","\n","\n","*   The twitter_samples contains subsets of 5,000 positive tweets, 5,000 negative tweets, and the full set of 10,000 tweets.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"eRlz8gJbPnNS","executionInfo":{"status":"ok","timestamp":1633618400100,"user_tz":-330,"elapsed":1023,"user":{"displayName":"CE020_Shivam_ Chandvaniya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03711213958246720098"}}},"source":["# select the set of positive and negative tweets\n","all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n","all_negative_tweets = twitter_samples.strings('negative_tweets.json')"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xoR1MPdqPpv5"},"source":["\n","\n","*   Train test split: 20% will be in the test set, and 80% in the training set.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"8EnrvEjAPs1A","executionInfo":{"status":"ok","timestamp":1633618421417,"user_tz":-330,"elapsed":2,"user":{"displayName":"CE020_Shivam_ Chandvaniya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03711213958246720098"}}},"source":["# split the data into two pieces, one for training and one for testing\n","#############################################################\n","test_pos = all_positive_tweets[4000:]\n","train_pos = all_positive_tweets[:4000]\n","test_neg = all_negative_tweets[4000:]\n","train_neg = all_negative_tweets[:4000]\n","train_x = train_pos + train_neg\n","test_x = test_pos + test_neg"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OXnxMpOmPvIA"},"source":["\n","\n","*  Create the numpy array of positive labels and negative labels.\n","\n"]},{"cell_type":"code","metadata":{"id":"uW79d6oAPx4Z","executionInfo":{"status":"ok","timestamp":1633618443575,"user_tz":-330,"elapsed":337,"user":{"displayName":"CE020_Shivam_ Chandvaniya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03711213958246720098"}}},"source":["# combine positive and negative labels\n","train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n","test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)\n","\n","Final_data = all_positive_tweets+all_negative_tweets\n","data =np.append(np.ones((len(all_positive_tweets), 1)), np.zeros((len(all_negative_tweets), 1)), axis=0)\n","train_x,test_x,train_y,test_y = train_test_split(Final_data,data,test_size=0.25,random_state= 26)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t2TIqGaIP0RA"},"source":["\n","\n","*   Create the frequency dictionary using the build_freqs() function.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RC7UcrSyP2SI","executionInfo":{"status":"ok","timestamp":1633618463958,"user_tz":-330,"elapsed":3971,"user":{"displayName":"CE020_Shivam_ Chandvaniya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03711213958246720098"}},"outputId":"caa3c495-3d36-423c-fed1-8e2848ed34ac"},"source":["# create frequency dictionary\n","#############################################################\n","freqs = build_freqs(train_x,train_y)\n","\n","# check the output\n","print(\"type(freqs) = \" + str(type(freqs)))\n","print(\"len(freqs) = \" + str(len(freqs.keys())))"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["type(freqs) = <class 'dict'>\n","len(freqs) = 10899\n"]}]},{"cell_type":"markdown","metadata":{"id":"2BKUF9QYP5g-"},"source":["\n","\n","*   HERE, The freqs dictionary is the frequency dictionary that's being built.\n","*   The key is the tuple (word, label), such as (\"happy\",1) or (\"happy\",0). The value stored for each key is the count of how many times the word \"happy\" was associated with a positive label, or how many times \"happy\" was associated with a negative label.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZA83mi9oQAJ-"},"source":["**Extracting the features**\n","\n","\n","*   Given a list of tweets, extract the features and store them in a matrix. You will extract two features.\n","\n","1.   The first feature is the number of positive words in a tweet.\n","2.  The second feature is the number of negative words in a tweet.\n","\n","\n","\n","\n","*  Then train your logistic regression classifier on these features.\n","\n","\n","*   Test the classifier on a validation set.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"TjkFCGKHQXny","executionInfo":{"status":"ok","timestamp":1633618601027,"user_tz":-330,"elapsed":3,"user":{"displayName":"CE020_Shivam_ Chandvaniya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03711213958246720098"}}},"source":["def extract_features(tweet, freqs):\n","    '''\n","    Input: \n","        tweet: a list of words for one tweet\n","        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n","    Output: \n","        x: a feature vector of dimension (1,3)\n","    '''\n","    # tokenizes, stems, and removes stopwords\n","    #############################################################\n","    word_l = process_tweet(tweet)\n","    \n","    # 3 elements in the form of a 1 x 3 vector\n","    x = np.zeros((1, 2)) \n","    \n","    #bias term is set to 1\n","    x[0,0] = 1 \n","        \n","    # loop through each word in the list of words\n","    for word in word_l:\n","        \n","        # increment the word count for the positive label 1\n","        #############################################################\n","        if((word,1) in freqs):\n","          x[0,0]+=freqs[word,1]\n","        # increment the word count for the negative label 0\n","        #############################################################\n","          if((word,0) in freqs):\n","            x[0,1]+=freqs[word,0]\n","        \n","    \n","    assert(x.shape == (1, 2))\n","    return x[0]"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"gfqV9uL1Qawq","executionInfo":{"status":"ok","timestamp":1633618610239,"user_tz":-330,"elapsed":523,"user":{"displayName":"CE020_Shivam_ Chandvaniya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03711213958246720098"}}},"source":["def predict_tweet(tweet):\n","  with tf.Session() as sess:\n","      saver.restore(sess,save_path='TSession')\n","      data_i=[]\n","      for t in tweet:\n","        data_i.append(extract_features(t,freqs))\n","      data_i=np.asarray(data_i)\n","      return sess.run(tf.nn.sigmoid(tf.add(tf.matmul(a=data_i,b=W,transpose_b=True),bias)))\n","      print(\"--Fail--\")\n","      return\n","  '''\n","    Input: \n","        tweet: a string\n","        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n","        theta: (3,1) vector of weights\n","    Output: \n","        y_pred: the probability of a tweet being positive or negative\n","    '''\n","    \n","    # extract the features of the tweet and store it into x\n","    #############################################################\n","    #x = extract_features(tweet,freqs)\n","    \n","    # make the prediction using x and theta\n","    #############################################################\n","    #y_pred = sigmoid(np.dot(x,theta))\n","    \n","    \n","    #return y_pred"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"IfszoXNbQd_Z","executionInfo":{"status":"ok","timestamp":1633618618848,"user_tz":-330,"elapsed":380,"user":{"displayName":"CE020_Shivam_ Chandvaniya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03711213958246720098"}}},"source":["bias=tf.Variable(np.random.randn(1),name=\"Bias\")\n","W=tf.Variable(np.random.randn(1,2),name=\"Weight\")"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"bfz8v0j6Qe-S","executionInfo":{"status":"ok","timestamp":1633618629436,"user_tz":-330,"elapsed":4167,"user":{"displayName":"CE020_Shivam_ Chandvaniya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03711213958246720098"}}},"source":["data=[]\n","for t in train_x:\n","  data.append(extract_features(t,freqs))\n","data=np.asarray(data)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mISMzKWTQhVa","executionInfo":{"status":"ok","timestamp":1633618635864,"user_tz":-330,"elapsed":363,"user":{"displayName":"CE020_Shivam_ Chandvaniya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03711213958246720098"}},"outputId":"2bb9dafb-a7fe-427c-f04a-40488eae62bc"},"source":["Y_hat = tf.nn.sigmoid(tf.add(tf.matmul(np.asarray(data), W,transpose_b=True), bias)) \n","ta=np.asarray(train_y)\n","Total_cost = tf.nn.sigmoid_cross_entropy_with_logits(logits = Y_hat, labels = ta) \n","print(Total_cost)"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor(\"logistic_loss:0\", shape=(7500, 1), dtype=float64)\n"]}]},{"cell_type":"code","metadata":{"id":"ZNlkfJICQjLw","executionInfo":{"status":"ok","timestamp":1633618643312,"user_tz":-330,"elapsed":337,"user":{"displayName":"CE020_Shivam_ Chandvaniya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03711213958246720098"}}},"source":["# Gradient Descent Optimizer \n","optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.00001 ,name=\"GradientDescent\").minimize(Total_cost) \n","# Global Variables Initializer \n","init = tf.global_variables_initializer()"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aTd4es1XQmav","executionInfo":{"status":"ok","timestamp":1633618664011,"user_tz":-330,"elapsed":11584,"user":{"displayName":"CE020_Shivam_ Chandvaniya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03711213958246720098"}},"outputId":"4e1948f7-7625-4f00-ad00-044a80695570"},"source":["saver = tf.train.Saver()\n","with tf.Session() as sess:\n","  \n","  sess.run(init)\n","  print(\"Bias\",sess.run(bias))\n","  print(\"Weight\",sess.run(W))\n","  for epoch in range(1000):\n","    sess.run(optimizer)\n","    preds=sess.run(Y_hat)\n","    acc=((preds==ta).sum())/len(train_y)\n","    Accuracy=[]\n","    repoch=False\n","    if repoch:\n","      Accuracy.append(acc)\n","    if epoch % 1000 == 0:\n","      print(\"Accuracy\",acc)\n","    saved_path = saver.save(sess, 'TSession')"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Bias [0.23226043]\n","Weight [[-0.94758381 -0.74812652]]\n","Accuracy 0.4472\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aTPA7pk1Qqcn","executionInfo":{"status":"ok","timestamp":1633618674668,"user_tz":-330,"elapsed":1568,"user":{"displayName":"CE020_Shivam_ Chandvaniya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03711213958246720098"}},"outputId":"4ce116ab-1384-474e-fc39-e0da3cda1665"},"source":["preds=predict_tweet(test_x)\n","print(preds,len(test_y))"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Restoring parameters from TSession\n","[[1.92057838e-52]\n"," [0.00000000e+00]\n"," [0.00000000e+00]\n"," ...\n"," [0.00000000e+00]\n"," [0.00000000e+00]\n"," [0.00000000e+00]] 2500\n"]}]},{"cell_type":"code","metadata":{"id":"U81uXrDFQs_n","executionInfo":{"status":"ok","timestamp":1633618680434,"user_tz":-330,"elapsed":700,"user":{"displayName":"CE020_Shivam_ Chandvaniya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03711213958246720098"}}},"source":["def accuracy(x,y):\n","  return ((x==y).sum())/len(y)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"SENRMa5rQuk4","executionInfo":{"status":"ok","timestamp":1633618686735,"user_tz":-330,"elapsed":785,"user":{"displayName":"CE020_Shivam_ Chandvaniya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03711213958246720098"}},"outputId":"e2465299-eb92-47fb-a201-33145c413dcc","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(accuracy(preds,test_y))"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["0.4488\n"]}]}]}